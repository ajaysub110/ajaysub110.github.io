<!DOCTYPE html>
<html lang="en">
	<head>
		<!-- Google tag (gtag.js) -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-TW9P59FKLZ"></script>
		<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-TW9P59FKLZ');
		</script>

		<title>Ajay Subramanian</title>
		<meta charset="UTF-8">
		<meta name="'viewport" content="width=device-width, initial-scale=1.0">
		<!-- Adobe Fonts (Typekit) -->
		<link rel="stylesheet" href="https://use.typekit.net/yvx8kro.css">
		<!-- Fallback Google Fonts -->
		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap" rel="stylesheet">
		<!-- Font Awesome for icons -->
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
		<link rel="stylesheet" href="stylesheet.css">
	</head>
	<body>
			<h1>Ajay Subramanian</h1>

		<!-- picture -->
		<img align="right" src="assets/ajay_pangonglake.jpeg" alt="Ajay Subramanian" width="300" style="margin-top: -60px;">

		<!-- position details -->
		<p>
		PhD student in Cognition & Perception<br>
		Department of Psychology<br>
			New York University<br>
		</p>

		<!-- contact icons -->
		<div class="contact-icons">
			<a href="https://goo.gl/maps/YFAWBm8CzGyug5Qh6" title="6 Washington Place, New York, NY 10003"><i class="fa-solid fa-location-dot"></i></a>
			<a href="mailto:ajay.subramanian@nyu.edu" title="Email"><i class="fa-solid fa-envelope"></i></a>
			<a href="https://x.com/ajaysub110" title="Twitter"><i class="fa-brands fa-x-twitter"></i></a>
			<a href="https://scholar.google.com/citations?user=6cyu_EgAAAAJ&hl=en" title="Google Scholar"><i class="fa-solid fa-graduation-cap"></i></a>
			<a href="https://github.com/ajaysub110" title="GitHub"><i class="fa-brands fa-github"></i></a>
			<a href="https://www.linkedin.com/in/ajaysub110/" title="LinkedIn"><i class="fa-brands fa-linkedin"></i></a>
		</div>

		<h3>Research</h3>
		<p>
			I am passionate about developing machine learning systems capable of robust, general, human-like visual perception. 
I have extensive experience in training and evaluating deep learning models, and am also proficient at designing and conducting online human psychophysics experiments. For deep learning, I have primarily used <a href="https://pytorch.org/">PyTorch</a> and <a href="https://www.tensorflow.org/">TensorFlow</a> to train both discriminative and generative models (with <a href="https://lightning.ai/docs/pytorch/stable/">PyTorch Lightning</a>, <a href="https://huggingface.co/docs/accelerate/en/index">Hugging Face Accelerate</a>, <a href="https://keras.io/">Keras</a>) and evaluate them (with <a href="https://github.com/Trusted-AI/adversarial-robustness-toolbox">ART</a>, <a href="https://github.com/bethgelab/model-vs-human">model-vs-human</a>, <a href="https://github.com/LabForComputationalVision/pyrtools">pyrtools</a>). I leverage tools such as <a href="https://lab.js.org/">lab.js</a> and <a href="https://www.psychopy.org/">PsychoPy</a> for human experiment design, and platforms like <a href="https://www.prolific.com/">Prolific</a>, <a href="https://pavlovia.org/">Pavlovia</a>, and <a href="https://www.mturk.com/">Amazon MTurk</a> for participant recruitment and data collection.
		</p>

		<h3>Publications</h3>
<ul class="publications">
    <li>
        <div class="pub-title">
            <a href="https://doi.org/10.1167/jov.25.1.4">Benchmarking the speedâ€“accuracy tradeoff in object recognition by humans and neural networks</a>
            <span class="pub-year">2025</span>
        </div>
        <span class="pub-authors">Subramanian, A. , Price, S. , Kumbhar, O. , Sizikova, E. , Majaj, N.  J. , Pelli, D.  G</span>
        <span class="pub-venue">Journal of Vision, 25(1), 4-4</span>
    </li>
    <li>
        <div class="pub-title">
            <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/0cdc1e85736d9c01d366cbf9b4b81672-Abstract-Conference.html">Spatial-frequency channels, shape bias, and adversarial robustness</a>
            <span class="pub-year">2024</span>
        </div>
        <span class="pub-authors">Subramanian, A. , Sizikova, E. , Majaj, N.  J. , Pelli, D.  G</span>
        <span class="pub-venue">Advances in neural information processing systems (NeurIPS), 36. [Oral presentation]</span>
    </li>
    <li>
        <div class="pub-title">
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608021003944">Reinforcement learning and its connections with neuroscience and psychology</a>
            <span class="pub-year">2022</span>
        </div>
        <span class="pub-authors">Subramanian, A. , Chitlangia, S. , Baths, V</span>
        <span class="pub-venue">Neural Networks, 145, 271-287</span>
    </li>
    <li>
        <div class="pub-title">
            <a href="https://iopscience.iop.org/article/10.1088/2634-4386/ac5ac5/meta">mlGeNN: accelerating SNN inference using GPU-enabled neural networks</a>
            <span class="pub-year">2022</span>
        </div>
        <span class="pub-authors">Turner, J.  P. , Knight, J.  C. , Subramanian, A. , Nowotny, T</span>
        <span class="pub-venue">Neuromorphic Computing and Engineering, 2(2), 024002</span>
    </li>
    <li>
        <div class="pub-title">
            <span>Human Learning of Complex Novel Tasks as Theory-Based Modeling, Exploration, and Planning</span>
            <span class="pub-year">2023</span>
        </div>
        <span class="pub-authors">Tsividis, P. , Loula, J. , Burga, J. , Rodriguez, J.  P. , Arnaud, S. , Foss, N. , Campero, A. , Subramanian, A. , Pouncy, T. , Gershman, S. , Tenenbaum, J.  B</span>
        <span class="pub-venue">Submitted to Philosophical Transactions of the Royal Society A. </span>
    </li>
    <li>
        <div class="pub-title">
            <a href="https://openreview.net/forum?id=O8nVUbyvTC#discussion">Revealing spatial-frequency channels in an ensemble encoding model of human fMRI</a>
            <span class="pub-year">2024</span>
        </div>
        <span class="pub-authors">Ozcelik, F. , Subramanian, A. , Majaj, N.  J. , Pelli, D.  G</span>
        <span class="pub-venue">Accepted to NeurIPS Workshop on Unifying Representations in Neural Models (UniReps)</span>
    </li>
    <li>
        <div class="pub-title">
            <a href="https://openreview.net/forum?id=mDr0o2WU2n">Comparing models of neural representation based on their metric tensors</a>
            <span class="pub-year">2023</span>
        </div>
        <span class="pub-authors">Zhou, J.  Y. , Chun, C. , Subramanian, A. , Simoncelli, E. , P</span>
        <span class="pub-venue">NeurIPS Workshop on Unifying Representations in Neural Models (UniReps)</span>
    </li>
    <li>
        <div class="pub-title">
            <span>Influence of background on the spatial-frequency channel for object recognition</span>
            <span class="pub-year">2024</span>
        </div>
        <span class="pub-authors">Subramanian, A. , Majaj, N. , Pelli, D.  G</span>
        <span class="pub-venue">Poster Presentation at Vision Science Society (VSS) Meeting. </span>
    </li>
    <li>
        <div class="pub-title">
            <span>Word2Brain2Image: Visual Reconstruction from Spoken Word Representations</span>
            <span class="pub-year">2019</span>
        </div>
        <span class="pub-authors">Subramanian, A. , Patil, R. , Baths, V</span>
        <span class="pub-venue">Poster Presentation at Annual Conference of the Association for Cognitive Science in India (ACCS). </span>
    </li>
</ul>
	</body>
</html>