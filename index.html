<!DOCTYPE html>
<html lang="en">
	<head>
		<!-- Google tag (gtag.js) -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-TW9P59FKLZ"></script>
		<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-TW9P59FKLZ');
		</script>

		<title>Ajay Subramanian</title>
		<meta charset="UTF-8">
		<meta name="'viewport" content="width=device-width, initial-scale=1.0">
		<link rel="stylesheet" href="stylesheet.css">
	</head>
	<body>
		<!-- PAGE HEADER -->
		<div class="page-header">
			<h1>Ajay Subramanian</h1>
		</div>

		<!-- CONTACT INFORMATION -->
		<div class="section-header">
			<h3>Contact information</h3>
		</div>

		<!-- picture -->
		<img align="right" src="assets/ajay_pangonglake.jpeg" alt="Ajay Subramanian" width="210">

		<!-- position details -->
		<b>Ajay Subramanian</b><br>
		PhD student in Cognition & Perception<br>
		Department of Psychology<br>
		New York University<br><br>

		Work address: <a href="https://goo.gl/maps/YFAWBm8CzGyug5Qh6">6 Washington Place, New York, NY 10003</a><br>
		<!-- email and social media -->
		Email: <a href="mailto:ajay.subramanian@nyu.edu">ajay.subramanian@nyu.edu</a><br>
		Twitter: <a href="https://twitter.com/ajaysub110">twitter.com/ajaysub110</a><br>
		Google Scholar: <a href="https://scholar.google.com/citations?user=6cyu_EgAAAAJ&hl=en">Ajay Subramanian</a><br>
		GitHub: <a href="https://github.com/ajaysub110">github.com/ajaysub110</a><br>
		LinkedIn: <a href="https://www.linkedin.com/in/ajaysub110/">linkedin.com/ajaysub110</a><br>
		<!-- <a href="assets/CV_AjaySubramanian.pdf">Curriculum Vitae</a> -->

		<!-- BIOS -->
		<div class="section-header">
			<h3>Research</h3>
		</div>
		<p>
			<!-- I am interested in studying how humans recognize objects in complex visual scenes, by blending psychophysical experiments with machine learning modeling. How do people identify, organize and use visual features to recognize objects? Classical psychophysical experiments have yielded insights into some of these questions, but often highly simplify the task or fall short in their explanations. My research focuses on using machine learning models, particularly deep neural networks, to a) bridge the gap between human psychophysics and model analysis, and b) extend simplistic results to more complex, real-world problems. -->
			<!-- I am interested in studying how humans and machines recognize objects in complex and natural visual scenes, by blending psychophysical methods with machine learning modeling. Psychophysics enables rapid experimentation to identify and model the brain's underlying computations. Machine learning modeling, particularly with deep neural networks, helps scale simple perceptual models to complex behavioral tasks and hence allow for better model-human comparison, while also yielding more robust, generalizable computational systems. -->
			I use psychophysical methods to probe the differences between human and machine vision. I am currently studying the computations that make human object recognition more robust than neural network recognition, towards building more robust deep learning systems. I am a PhD student in Cognition & Perception at New York University, advised by <a href="https://denispelli.com/">Denis Pelli</a>. Previously, I completed my undergraduate degree in Electronics and Communication Engineering at BITS Pilani, India. My broad interests are in understanding and developing deep learning systems using ideas from human psychology and neuroscience. 
			<!-- <ul>
				<li><b>Computational neuroscience:</b> Worked with <a href="https://www.bits-pilani.ac.in/goa/basabdattab/profile">Dr. Basabdatta Sen Bhattacharya</a> on a spiking neural network model of the LGN and with <a href="https://profiles.sussex.ac.uk/p415734-james-knight">Dr. Jamie Knight</a> and <a href="https://profiles.sussex.ac.uk/p206151-thomas-nowotny">Dr. Thomas Nowotny</a> on converting trained deep networks to equivalent energy-efficient GPU-enabled spiking neural networks.</li>
				<li><b>Cognitive neuroscience:</b> Worked with <a href="https://www.bits-pilani.ac.in/goa/veeky/profile">Dr. Veeky Baths</a> on decoding visual representations of spoken words in the brain.</li>
				<li><b>Cognitive science:</b> Worked with <a href="https://psychology.fas.harvard.edu/people/samuel-j-gershman">Dr. Samuel Gershman</a> and <a href="https://pedrotsividis.com/">Dr. Pedro Tsividis</a> on theory-based reinforcement learning.</li>
				<li><b>Applied ML:</b> With <a href="https://www.cshl.edu/research/faculty-staff/partha-mitra/">Dr. Partha Mitra</a> and <a href="https://scholar.google.co.in/citations?user=Z_fWZ8oAAAAJ&hl=en">Dr. Jaikishan Jayakumar</a> on deep-learning-based segmentation of gigapixel neuroanatomical images.</li>
			</ul> -->
		</p>

		<!-- PAPERS -->
		<!-- <div class="section-header">
			<h3>Papers</h3>
		</div> -->
		<b>Publications</b><br>
		<ul>
			<li>
				<b>Subramanian, A.</b>, Sizikova, E., Majaj, N. J., Pelli, D. G. (2023).
				<a href="https://arxiv.org/abs/2309.13190">Spatial-frequency channels, shape bias, and adversarial robustness.</a>
				<i>To appear in Advances in neural information processing systems (NeurIPS), 36</i>
			</li>
			<li>
				<b>Subramanian, A.</b>, Chitlangia, S., Baths, V. (2022).
				<a href="https://www.sciencedirect.com/science/article/pii/S0893608021003944">Reinforcement learning and its connections with neuroscience and psychology.</a>
				<i>Neural Networks, 145</i>, 271-287.
			</li>
			<li>
				Turner, J. P., Knight, J. C., <b>Subramanian, A.</b>, Nowotny, T. (2022).
				<a href="https://iopscience.iop.org/article/10.1088/2634-4386/ac5ac5/meta">mlGeNN: accelerating SNN inference using GPU-enabled neural networks.</a>
				<i>Neuromorphic Computing and Engineering, 2</i>(2), 024002.
			</li>
		</ul>

		<b>Submitted papers and Preprints</b><br>
		<ul>
			<!-- <li>
				Zhou, J. Y., <b>Subramanian, A.</b>, Chun, C. (2023).
				How does perceptual discriminability relate to neuronal receptive fields?
				<i>Under review in Journal of Vision.</i>
			</li> -->
			<li>
				Zhou, J. Y., Chun, C., <b>Subramanian, A.</b>, Simoncelli, E. P. (2023).
				Comparing models of neural representation based on their metric tensors.
				<i>In preparation.</i>	
			</li>
			<li>
				Tsividis, P., Loula, J., Burga, J., Rodriguez, J. P., Arnaud, S., Foss, N., Campero, A., <b>Subramanian, A.</b>, Pouncy, T., Gershman, S., Tenenbaum, J. B. (2023).
				Human Learning of Complex Novel Tasks as Theory-Based Modeling, Exploration, and Planning.
				<i>Under review in PNAS.</i>
			</li>
			<li>
				<b>Subramanian, A.</b>, Price, S., Sizikova, E., Kumbhar, O., Majaj, N. J., Pelli, D. G. (<i>arXiv</i>, 2022).
				<a href="https://arxiv.org/abs/2206.08427">SATBench: Benchmarking the speed-accuracy tradeoff in object recognition by humans and dynamic neural networks.</a>
				<i>arXiv</i>.
			</li>
		</ul>

		<b>Conference Talks</b><br>
		<ul>
			<!-- <li>
				<b>Subramnian, A.</b>, Sizikova, E., Majaj, N. J., Pelli, D. G. (2003).
				Spatial-frequency channels, shape bias, and adversarial robustness.
				<i>To appear as Oral presentation, NeurIPS,</i> New Orleans, USA
			</li> -->
			<li>
				<b>Subramnian, A.</b>, Patil, R., Baths, V. (2020).
				Word2Brain2Image: A data-driven approach towards understanding representations in the brain.
				<i>Round table track: Data issues in Cognitive Neuroscience, International CCCP Symposium.</i> Virtual.
			</li>
		</ul>


		<b>Conference Posters</b><br>
		<ul>
			<!-- <li>
				<b>Subramnian, A.</b>, Sizikova, E., Majaj, N. J., Pelli, D. G. (2003).
				Spatial-frequency channels, shape bias, and adversarial robustness.
				<i>To appear in NeurIPS,</i> New Orleans, USA
			</li> -->
			<li>
				<b>Subramanian, A.</b>, Sizikova, E., Majaj, N. J., Pelli, D. G. (2023).
				Spatial-frequency channels for object recognition by neural networks are twice as wide as those of humans. An explanation for shape bias?
				<i>ECVP</i>, Paphos, Cyprus.
			</li>
			<li>
				<b>Subramanian, A.</b>, Sizikova, E., Majaj, N. J., Pelli, D. G. (2023).
				Spatial-frequency channels for object recognition by neural networks are twice as wide as those of humans.
				<i>VSS Meeting</i>, St. Pete Beach, USA.
			</li>
			<li>
				Zhou, J. Y., Chun, C., <b>Subramanian, A.</b>, Simoncelli, E., P. (2023).
				Computing and comparing metric tensors in neural response models.
				<i>VSS Meeting</i>, St. Pete Beach, USA.
			</li>
			<li>
				<b>Subramanian, A.</b>, Sizikova, E., Majaj, N. J., Pelli, D. G. (2023).
				Spatial-frequency channels for object recognition by neural networks are twice as wide as those of humans.
				<i>COSYNE</i>, Montreal, Canada.
			</li>
			<li>
				<b>Subramanian, A.</b>, Price, S., Sizikova, E., Kumbhar, O., Majaj, N., Pelli, D. G. (2022).
				Benchmarking dynamic neural-network models of the human speed-accuracy tradeoff.
				<i>VSS Meeting</i>, St. Pete Beach, USA.
			</li>
			<li>
				<b>Subramanian, A.</b>, Patil, R., Baths, V. (2022).
				Word2Brain2Image: Visual Reconstruction from Spoken Word Representations.
				<i>ACCS</i>, Goa, India.
			</li>
		</ul>

	</body>
</html>